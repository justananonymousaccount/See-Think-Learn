<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="">
  <meta property="og:title" content="See, Think, Learn"/>
  <meta property="og:description" content="A Self-Taught Multimodal Reasoner"/>
  <meta property="og:url" content="https://github.com/justananonymousaccount/See-Think-Learn/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630
  <meta property="og:image" content="" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->

<!-- 
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multimodal Large Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>See-Think-Learn</title>
  <link rel="icon" type="" href="static/images/logo_transp2.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/tab_gallery.css">
  <link rel="stylesheet" href="static/css/juxtapose.css">
  <link rel="stylesheet" href="static/css/image_card_fader.css">
  <link rel="stylesheet" href="static/css/image_card_slider.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          
          <!-- Logo added here -->
          <img src="static/images/logo4.png" alt="Logo" style="max-height: 200px; margin-bottom: 1rem;">

          <h1 class="title is-1 publication-title">See, Think, Learn</h1>
          <h1 class="title is-3 publication-title">A Self-Taught Multimodal Reasoner</h1>           
          
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">ABC</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Institution Name<br>Anonymous Institute</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">

              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2512.02456v1" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- Supplementary PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2512.02456v1" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Supplementary</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/justananonymousaccount/STL-Implementation/" target="_blank"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

            </div>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>








<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Vision-Language Models (VLMs) have achieved remarkable progress in integrating visual perception with language
understanding. However, effective multimodal reasoning
requires both accurate perception and robust reasoning,
and weakness in either limits the performance of VLMs.
Prior efforts to enhance reasoning often depend on highquality chain-of-thought (CoT) data, obtained via laborintensive human annotations, costly proprietary models or
self-training methods that overlook perception. To address
these limitations, we propose a simple yet effective selftraining framework called "See-Think-Learn" (STL). At its
core, STL introduces a structured reasoning template that
encourages the model to see before thinking: first extracting visual attributes in textual form, then using them to
guide reasoning. The framework jointly improves perception and reasoning by having the model generate and learn
from its own structured rationales in a self-training loop.
Furthermore, we augment the training data with negative
rationales, i.e. explanations that justify why certain answer
choices are incorrect, to enhance the modelâ€™s ability to distinguish between correct and misleading responses. This
fosters more discriminative and robust learning. Experiments across diverse domains show that STL consistently
outperforms baselines trained directly only on answers or
self-generated reasoning, while qualitative analysis confirms
the high quality of its rationales. STL thus provides a costeffective solution to enhance multimodal reasoning ability
of VLMs. We will make the code publicly available upon
acceptance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">

      <!--=================Results==========================-->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Visualization</h2>
          <div class="content has-text-justified">
            <div style="text-align: center; vertical-align:middle">
              <img src="static/images/prompt.png" width="800">
            </div>
            <div align="center">
              <b>Visualization of Positive and Negative Rationales from the proposed STL.</b> The example illustrates correct identification and reasoning for the chosen answer and rejection of an incorrect alternative.
            </div>
        </div>

    </div>
  </div>
  </div>
  </section>

<section class="section">
  <!--=================Results==========================-->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Subjective Evaluation</h2>
      <div class="content has-text-justified">
      <div style="text-align: center; vertical-align:middle">
        <img src="static/images/bar.png" width="800">
      </div>
      
      <div align="center">
        <b>Comparison of Rank Distributions by Rank Category Across Methods and Domains.</b> Each subplot displays counts of Rank 1, Rank 2, and Rank 3 judgments (out of 150) for Human, STaR, and STL (Ours) methods within a domain. Bars are grouped by rank category to illustrate how each method performs across different ranks. Notice that across all domains, the reasoning generated by our method is preferred for more samples than that of STaR. This highlights the fact that the reasoning generated by our method is of superior quality compared to that of STaR.
      </div>
  </div>
</div>
</div>
</div>
</section>




  



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>Yet to come!</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            Website adapted from the source code <a href="https://github.com/nerfies/nerfies.github.io"> Nerfies</a>.
          <br><br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
