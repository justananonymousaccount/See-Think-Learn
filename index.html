<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="">
  <meta property="og:title" content="See, Think, Learn"/>
  <meta property="og:description" content="A Self-Taught Multimodal Reasoner"/>
  <meta property="og:url" content="https://github.com/justananonymousaccount/See-Think-Learn/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630
  <meta property="og:image" content="" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/> -->

<!-- 
  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG"> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multimodal Large Language Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>See, Think, Learn</title>
  <link rel="icon" type="" href="static/images/logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="static/css/tab_gallery.css">
  <link rel="stylesheet" href="static/css/juxtapose.css">
  <link rel="stylesheet" href="static/css/image_card_fader.css">
  <link rel="stylesheet" href="static/css/image_card_slider.css">


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">See, Think, Learn</h1>
            <h1 class="title is-3 publication-title">A Self-Taught Multimodal Reasoner</h1>           
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">ABC</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Anonymous Institute</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://github.com/justananonymousaccount/See-Think-Learn/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="https://github.com/justananonymousaccount/See-Think-Learn/" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/justananonymousaccount/STL-Implementation/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>







<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
Chain-of-Thought (CoT) prompting has enabled Large Language Models (LLMs) to achieve impressive reasoning capabilities, particularly in complex textual tasks. However, extending these capabilities to multimodal reasoning remains a significant challenge. Existing Vision-Language Models (VLMs) often struggle with effective visual understanding and reasoning, resulting in suboptimal performance on complex tasks. Current efforts to induce rationale generation in VLMs typically rely on constructing large-scale rationale datasets, often through human annotations or proprietary models, which are both costly. To address these limitations, we propose a simple yet effective self-training framework called "See-Think-Learn" (STL). At its core, STL introduces a structured reasoning template that encourages the model to <it>see before thinking</it> by explicitly extracting visual attributes in textual form before proceeding to reason about the task. The framework iteratively improves both visual perception and multimodal reasoning by having the model generate and learn from its own structured rationales. Furthermore, we augment the training data with negative rationales, i.e. explanations that justify why certain answer choices are incorrect, to enhance the model's ability to distinguish between correct and misleading responses. This fosters more discriminative and robust learning. Experiments across diverse domains demonstrate that STL significantly improves performance by leveraging the modelâ€™s own perception and reasoning. It surpasses models fine-tuned only on final answers and performs comparably to those trained on human-annotated rationales.  Thus, our approach enables the model to improve itself by learning from its own generated perceptions and reasoning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">

      <!--=================Results==========================-->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Visualization</h2>
          <div class="content has-text-justified">
            <div style="text-align: center; vertical-align:middle">
              <img src="static/images/prompt.png" width="800">
            </div>
            <div align="center">
              <b>Visualization of Positive and Negative Rationales from the proposed STL.</b>The example illustrates correct identification and reasoning for the chosen answer and rejection of an incorrect alternative.
            </div>
        </div>

    </div>
  </div>
  </div>
  </section>

<section class="section">
  <!--=================Results==========================-->
  <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">Subjective Evaluation</h2>
      <div class="content has-text-justified">
      <div style="text-align: center; vertical-align:middle">
        <img src="static/images/bar.png" width="800">
      </div>
      
      <div align="center">
        <b>Comparison of Rank Distributions by Rank Category Across Methods and Domains.</b> Each subplot displays counts of Rank 1, Rank 2, and Rank 3 judgments (out of 150) for Human, STaR, and STL (Ours) methods within a domain. Bars are grouped by rank category to illustrate how each method performs across different ranks. Notice that across all domains, the reasoning generated by our method is preferred for more samples than that of STaR. This highlights the fact that the reasoning generated by our method is of superior quality compared to that of STaR.
      </div>
  </div>
</div>
</div>
</div>
</section>




  



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>Yet to come!</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            Website adapted from the source code <a href="https://github.com/nerfies/nerfies.github.io"> Nerfies</a>.
          <br><br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  </body>
  </html>
